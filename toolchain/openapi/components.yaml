schemas:
  ARANGO_ERROR:
    description: An ArangoDB Error code
    type: integer
  ArangoError:
    description: the arangodb error type
    properties:
      code:
        description: the HTTP Status code
        type: integer
      error:
        description: boolean flag to indicate whether an error occurred (*true* in
          this case)
        type: boolean
      errorMessage:
        description: a descriptive error message describing what happened, may contain
          additional information
        type: string
      errorNum:
        description: the ARANGO_ERROR code
        type: integer
  computed_field:
    description: ''
    properties:
      computeOn:
        description: |+
          An array of strings that defines on which write operations the value is
          computed. The possible values are `"insert"`, `"update"`, and `"replace"`.

        format: string
        items:
          type: string
        type: array
      expression:
        description: |+
          An AQL `RETURN` operation with an expression that computes the desired value.

        type: string
      failOnWarning:
        description: |+
          Whether the write operation fails if the expression produces a warning.

        type: boolean
      keepNull:
        description: |+
          Whether the target attribute is set if the expression evaluates to `null`.

        type: boolean
      name:
        description: |+
          The name of the target attribute.

        type: string
      overwrite:
        description: |+
          Whether the computed value takes precedence over a user-provided or
          existing attribute.

        type: boolean
    type: object
  edge_representation:
    description: |+
      The complete deleted edge document.
      Includes all attributes stored before this operation.
      Will only be present if returnOld is true.

    properties:
      _from:
        description: |+
          The _from value of the stored data.

        type: string
      _id:
        description: |+
          The _id value of the stored data.

        type: string
      _key:
        description: |+
          The _key value of the stored data.

        type: string
      _rev:
        description: |+
          The _rev value of the stored data.

        type: string
      _to:
        description: |+
          The _to value of the stored data.

        type: string
    type: object
  vertex_representation:
    description: |+
      The internal attributes for the vertex.

    properties:
      _id:
        description: |+
          The _id value of the stored data.

        type: string
      _key:
        description: |+
          The _key value of the stored data.

        type: string
      _rev:
        description: |+
          The _rev value of the stored data.

        type: string
    required:
    - vertex
    type: object
  move_shard_operation:
    description: ''
    properties:
      collection:
        description: |+
          Collection ID of the collection the shard belongs to.

        format: ''
        type: number
      from:
        description: |+
          The server name from which to move.

        type: string
      isLeader:
        description: |+
          True if this is a leader move shard operation.

        type: boolean
      shard:
        description: |+
          Shard ID of the shard to be moved.

        type: string
      to:
        description: |+
          The ID of the destination server.

        type: string
    type: object
  get_api_control_pregel:
    description: |2+

    properties:
      algorithm:
        description: |+
          The algorithm used by the job.

        type: string
      computationTime:
        description: "The algorithm execution time. Is shown when the computation\
          \ started. \n\n"
        format: float
        type: number
      created:
        description: |+
          The date and time when the job was created.

        type: string
      detail:
        $ref: '#/components/schemas/get_api_control_pregel_detail'
      edgeCount:
        description: |+
          The total number of edges processed.

        format: int64
        type: integer
      expires:
        description: |+
          The date and time when the job results expire. The expiration date is only
          meaningful for jobs that were completed, canceled or resulted in an error. Such jobs
          are cleaned up by the garbage collection when they reach their expiration date/time.

        type: string
      gss:
        description: |+
          The number of global supersteps executed.

        format: int64
        type: integer
      gssTimes:
        description: |+
          Computation time of each global super step. Is shown when the computation started.

        format: number
        items:
          type: number
        type: array
      id:
        description: |+
          The ID of the Pregel job, as a string.

        type: string
      reports:
        description: |+
          This attribute is used by Programmable Pregel Algorithms (`air`, experimental).
          The value is only populated once the algorithm has finished.

        format: object
        items:
          type: object
        type: array
      startupTime:
        description: |+
          The startup runtime of the execution.
          The startup time includes the data loading time and can be substantial.

        format: float
        type: number
      state:
        description: "The state of the execution. The following values can be returned:\n\
          - `\"none\"`: The Pregel run did not yet start.\n- `\"loading\"`: The graph\
          \ is loaded from the database into memory before the execution of the algorithm.\n\
          - `\"running\"`: The algorithm is executing normally.\n- `\"storing\"`:\
          \ The algorithm finished, but the results are still being written\n  back\
          \ into the collections. Occurs only if the store parameter is set to true.\n\
          - `\"done\"`: The execution is done. In version 3.7.1 and later, this means\
          \ that\n  storing is also done. In earlier versions, the results may not\
          \ be written back\n  into the collections yet. This event is announced in\
          \ the server log (requires\n  at least info log level for the `pregel` log\
          \ topic).\n- `\"canceled\"`: The execution was permanently canceled, either\
          \ by the user or by\n  an error.\n- `\"fatal error\"`: The execution has\
          \ failed and cannot recover.\n- `\"in error\"`: The execution is in an error\
          \ state. This can be\n  caused by DB-Servers being not reachable or being\
          \ non responsive. The execution\n  might recover later, or switch to `\"\
          canceled\"` if it was not able to recover\n  successfully. \n- `\"recovering\"\
          ` (currently unused): The execution is actively recovering and\n  switches\
          \ back to `running` if the recovery is successful.\n\n"
        type: string
      storageTime:
        description: |+
          The time for storing the results if the job includes results storage.
          Is shown when the storing started.

        format: float
        type: number
      totalRuntime:
        description: |+
          The total runtime of the execution up to now (if the execution is still ongoing).

        format: float
        type: number
      ttl:
        description: |+
          The TTL (time to live) value for the job results, specified in seconds.
          The TTL is used to calculate the expiration date for the job's results.

        format: float
        type: number
      vertexCount:
        description: |+
          The total number of vertices processed.

        format: int64
        type: integer
    required:
    - detail
    type: object
  get_api_control_pregel_detail:
    description: |+
      The Pregel run details.

    properties:
      aggregatedStatus:
        $ref: '#/components/schemas/get_api_control_pregel_detail_aggregated'
      workerStatus:
        description: |
          The details of the Pregel for every DB-Server. Each object key is a DB-Server ID,
          and each value is a nested object similar to the `aggregatedStatus` attribute.
          In a single server deployment, there is only a single entry with an empty string as key.
        format: ''
        type: object
    required:
    - aggregatedStatus
    type: object
  get_api_control_pregel_detail_aggregated:
    description: |+
      The aggregated details of the full Pregel run. The values are totals of all the
      DB-Server.

    properties:
      allGssStatus:
        $ref: '#/components/schemas/get_api_control_pregel_detail_aggregated_gss'
      graphStoreStatus:
        $ref: '#/components/schemas/get_api_control_pregel_detail_aggregated_store'
      timeStamp:
        description: |+
          The time at which the status was measured.

        type: string
    type: object
  get_api_control_pregel_detail_aggregated_gss:
    description: |+
      Information about the global supersteps.

    properties:
      items:
        description: |+
          A list of objects with details for each global superstep.

        format: get_api_control_pregel_detail_aggregated_gss_items
        items:
          $ref: '#/components/schemas/get_api_control_pregel_detail_aggregated_gss_items'
        type: array
    type: object
  get_api_control_pregel_detail_aggregated_gss_items:
    description: ''
    properties:
      memoryBytesUsedForMessages:
        description: |+
          The number of bytes used in memory for the messages in this step.

        format: int64
        type: integer
      messagesReceived:
        description: |+
          The number of messages received in this step.

        format: int64
        type: integer
      messagesSent:
        description: |+
          The number of messages sent in this step.

        format: int64
        type: integer
      verticesProcessed:
        description: |+
          The number of vertices that have been processed in this step.

        format: int64
        type: integer
    type: object
  get_api_control_pregel_detail_aggregated_store:
    description: |+
      The status of the in memory graph.

    properties:
      edgesLoaded:
        description: |+
          The number of edges that are loaded from the database into memory.

        format: int64
        type: integer
      memoryBytesUsed:
        description: |+
          The number of bytes used in-memory for the loaded graph.

        format: int64
        type: integer
      verticesLoaded:
        description: |+
          The number of vertices that are loaded from the database into memory.

        format: int64
        type: integer
      verticesStored:
        description: |+
          The number of vertices that are written back to the database after the Pregel
          computation finished. It is only set if the `store` parameter is set to `true`.

        format: int64
        type: integer
    type: object
  graph_edge_definition:
    description: ''
    properties:
      collection:
        description: |+
          Name of the edge collection, where the edge are stored in.

        type: string
      from:
        description: |+
          List of vertex collection names.
          Edges in collection can only be inserted if their _from is in any of the collections here.

        format: string
        items:
          type: string
        type: array
      to:
        description: |
          List of vertex collection names.
          Edges in collection can only be inserted if their _to is in any of the collections here.
        format: string
        items:
          type: string
        type: array
    type: object
  key_generator_type:
    description: |+
      A object which contains key generation options

    properties:
      allowUserKeys:
        description: |+
          if set to `true`, then it is allowed to supply
          own key values in the `_key` attribute of a document. If set to
          `false`, then the key generator is solely responsible for
          generating keys and supplying own key values in the `_key` attribute
          of documents is considered an error.

        type: boolean
      lastValue:
        description: |2+

        format: ''
        type: integer
      type:
        description: |+
          specifies the type of the key generator. The currently
          available generators are `traditional`, `autoincrement`, `uuid`
          and `padded`.

        type: string
    type: object
  get_api_query_rules:
    description: ''
    properties:
      flags:
        type: object
        description: |+
          An object with the properties of the rule.

        format: get_api_query_rules_flags
      name:
        type: string
        description: |+
          The name of the optimizer rule as seen in query explain outputs.

        format: ''
    required:
    - flags
    type: object
  get_api_query_rules_flags:
    description: |+
      An object with the properties of the rule.

    properties:
      canBeDisabled:
        type: boolean
        description: |+
          Whether users are allowed to disable this rule. A few rules are mandatory.

        format: ''
      canCreateAdditionalPlans:
        type: boolean
        description: |+
          Whether this rule may create additional query execution plans.

        format: ''
      clusterOnly:
        type: boolean
        description: |+
          Whether the rule is applicable in the cluster deployment mode only.

        format: ''
      disabledByDefault:
        type: boolean
        description: |+
          Whether the optimizer considers this rule by default.

        format: ''
      enterpriseOnly:
        description: |+
          Whether the rule is available in the Enterprise Edition only.

        type: boolean
      hidden:
        type: boolean
        description: |+
          Whether the rule is displayed to users. Internal rules are hidden.

        format: ''
    type: object
  api_task_struct:
    description: |+
      The function in question

    properties:
      command:
        description: |+
          the javascript function for this task

        type: string
      created:
        description: |+
          The timestamp when this task was created

        format: float
        type: number
      database:
        description: |
          the database this task belongs to
        type: string
      id:
        description: |+
          A string identifying the task

        type: string
      name:
        description: |+
          The fully qualified name of the user function

        type: string
      offset:
        description: |+
          time offset in seconds from the created timestamp

        format: float
        type: number
      period:
        description: |+
          this task should run each `period` seconds

        format: ''
        type: number
      type:
        description: |+
          What type of task is this [ `periodic`, `timed`]
            - periodic are tasks that repeat periodically
            - timed are tasks that execute once at a specific time

        type: string
    type: object
  collection_info:
    description: |2+

    properties:
      cacheEnabled:
        description: |+
          Whether the in-memory hash cache for documents is enabled for this
          collection.

        type: boolean
      computedValues:
        description: |+
          A list of objects, each representing a computed value.

        format: computed_field
        items:
          $ref: '#/components/schemas/computed_field'
        type: array
      globallyUniqueId:
        description: |
          Unique identifier of the collection
        type: string
      id:
        description: |+
          unique identifier of the collection; *deprecated*

        type: string
      isSmart:
        description: |+
          Whether the collection is used in a SmartGraph (Enterprise Edition only).
          _(cluster only)_

        type: boolean
      isSystem:
        description: |+
          true if this is a system collection; usually `name` will start with an underscore.

        type: boolean
      keyOptions:
        $ref: '#/components/schemas/key_generator_type'
      name:
        description: |+
          literal name of this collection

        type: string
      numberOfShards:
        description: |+
          The number of shards of the collection. _(cluster only)_

        format: ''
        type: integer
      replicationFactor:
        description: |+
          contains how many copies of each shard are kept on different DB-Servers.
          It is an integer number in the range of 1-10 or the string `"satellite"`
          for a SatelliteCollection (Enterprise Edition only). _(cluster only)_

        format: ''
        type: integer
      schema:
        description: |+
          The collection level schema for documents.

        format: ''
        type: object
      shardKeys:
        description: |+
          contains the names of document attributes that are used to
          determine the target shard for documents. _(cluster only)_

        format: string
        items:
          type: string
        type: array
      shardingStrategy:
        description: |+
          the sharding strategy selected for the collection.
          One of 'hash' or 'enterprise-hash-smart-edge'. _(cluster only)_

        type: string
      smartGraphAttribute:
        description: |+
          Attribute that is used in SmartGraphs (Enterprise Edition only). _(cluster only)_

        type: string
      smartJoinAttribute:
        description: |+
          Determines an attribute of the collection that must contain the shard key value
          of the referred-to SmartJoin collection (Enterprise Edition only). _(cluster only)_

        type: string
      type:
        description: |+
          The type of the collection:
            - `0`: "unknown"
            - `2`: regular document collection
            - `3`: edge collection

        format: ''
        type: integer
      waitForSync:
        description: |+
          If `true` then creating, changing or removing
          documents will wait until the data has been synchronized to disk.

        type: boolean
      writeConcern:
        description: |+
          determines how many copies of each shard are required to be
          in sync on the different DB-Servers. If there are less then these many copies
          in the cluster a shard will refuse to write. Writes to shards with enough
          up-to-date copies will succeed at the same time however. The value of
          `writeConcern` cannot be larger than `replicationFactor`. _(cluster only)_

        format: ''
        type: integer
    required:
    - keyOptions
    type: object
  rebalance_compute:
    description: |2+


    properties:
      databasesExcluded:
        description: |+
          A list of database names to exclude from the analysis. (Default: `[]`)

        format: string
        items:
          type: string
        type: array
      leaderChanges:
        description: |+
          Allow leader changes without moving data. (Default: `true`)

        type: boolean
      maximumNumberOfMoves:
        description: |+
          Maximum number of moves to be computed. (Default: `1000`)

        format: ''
        type: number
      moveFollowers:
        description: |+
          Allow moving followers. (Default: `false`)

        type: boolean
      moveLeaders:
        description: |+
          Allow moving leaders. (Default: `false`)

        type: boolean
      piFactor:
        description: |+
          (Default: `256e6`)

        format: ''
        type: number
      version:
        description: |+
          Must be set to `1`.

        format: ''
        type: number
    required:
    - ''
    type: object
  explain_options:
    type: object
    properties:
      allPlans:
        type: boolean
        description: |+
          if set to *true*, all possible execution plans will be returned.
          The default is *false*, meaning only the optimal plan will be returned.

        format: ''
      maxNumberOfPlans:
        type: integer
        description: |+
          an optional maximum number of plans that the optimizer is
          allowed to generate. Setting this attribute to a low value allows to put a
          cap on the amount of work the optimizer does.

        format: int64
      optimizer:
        type: object
        description: |+
          Options related to the query optimizer.

        format: explain_options_optimizer
  explain_options_optimizer:
    type: object
    properties:
      rules:
        type: array
        description: |+
          A list of to-be-included or to-be-excluded optimizer rules can be put into this
          attribute, telling the optimizer to include or exclude specific rules. To disable
          a rule, prefix its name with a `-`, to enable a rule, prefix it with a `+`. There is
          also a pseudo-rule `all`, which matches all optimizer rules. `-all` disables all rules.

        items:
          type: string
  jwt_secret_struct:
    type: object
    properties:
      active:
        type: object
        description: |+
          An object with the SHA-256 hash of the active secret.

        format: ''
      passive:
        type: array
        description: |+
          An array of objects with the SHA-256 hashes of the passive secrets.
          Can be empty.

        items:
          type: object
  cluster_endpoints_struct:
    type: object
    properties:
      endpoint:
        type: string
        description: |+
          The bind of the Coordinator, like `tcp://[::1]:8530`

        format: ''
  version_details_struct:
    type: object
    properties:
      architecture:
        type: string
        description: |+
          The CPU architecture, i.e. *64bit*

        format: ''
      arm:
        type: string
        description: |+
          *false* - this is not running on an ARM cpu

        format: ''
      asan:
        type: string
        description: |+
          has this been compiled with the asan address sanitizer turned on? (should be false)

        format: ''
      asm-crc32:
        type: string
        description: |+
          do we have assembler implemented CRC functions?

        format: ''
      assertions:
        type: string
        description: |+
          do we have assertions compiled in (=> developer version)

        format: ''
      boost-version:
        type: string
        description: |+
          which boost version do we bind

        format: ''
      build-date:
        type: string
        description: |+
          the date when this binary was created

        format: ''
      build-repository:
        type: string
        description: |+
          reference to the git-ID this was compiled from

        format: ''
      compiler:
        type: string
        description: |+
          which compiler did we use

        format: ''
      cplusplus:
        type: string
        description: |+
          C++ standards version

        format: ''
      debug:
        type: string
        description: |+
          *false* for production binaries

        format: ''
      endianness:
        type: string
        description: |+
          currently only *little* is supported

        format: ''
      failure-tests:
        type: string
        description: |+
          *false* for production binaries (the facility to invoke fatal errors is disabled)

        format: ''
      fd-client-event-handler:
        type: string
        description: |+
          which method do we use to handle fd-sets, *poll* should be here on linux.

        format: ''
      fd-setsize:
        type: string
        description: |+
          if not *poll* the fd setsize is valid for the maximum number of filedescriptors

        format: ''
      full-version-string:
        type: string
        description: |+
          The full version string

        format: ''
      icu-version:
        type: string
        description: |+
          Which version of ICU do we bundle

        format: ''
      jemalloc:
        type: string
        description: |+
          *true* if we use jemalloc

        format: ''
      maintainer-mode:
        type: string
        description: |+
          *false* if this is a production binary

        format: ''
      openssl-version:
        type: string
        description: |+
          which openssl version do we link?

        format: ''
      platform:
        type: string
        description: |+
          the host os - *linux*, *windows* or *darwin*

        format: ''
      reactor-type:
        type: string
        description: |+
          *epoll* TODO

        format: ''
      rocksdb-version:
        type: string
        description: |+
          the rocksdb version this release bundles

        format: ''
      server-version:
        type: string
        description: |+
          the ArangoDB release version

        format: ''
      sizeof int:
        type: string
        description: |+
          number of bytes for *integers*

        format: ''
      sizeof void*:
        type: string
        description: |+
          number of bytes for *void pointers*

        format: ''
      sse42:
        type: string
        description: |+
          do we have a SSE 4.2 enabled cpu?

        format: ''
      unaligned-access:
        type: string
        description: |+
          does this system support unaligned memory access?

        format: ''
      v8-version:
        type: string
        description: |+
          the bundled V8 javascript engine version

        format: ''
      vpack-version:
        type: string
        description: |+
          the version of the used velocypack implementation

        format: ''
      zlib-version:
        type: string
        description: |+
          the version of the bundled zlib

        format: ''
      mode:
        type: string
        description: |+
          the mode we're runnig as - one of [*server*, *console*, *script*]

        format: ''
  admin_echo_server_struct:
    type: object
    properties:
      address:
        type: string
        description: |+
          The bind address of the endpoint this request was sent to

        format: ''
      port:
        type: integer
        description: |+
          The port this request was sent to

        format: ''
      endpoint:
        type: string
        description: |+
          The endpoint this request was sent to

        format: ''
  admin_echo_client_struct:
    type: object
    properties:
      address:
        type: integer
        description: |+
          The IP address of the client

        format: ''
      port:
        type: integer
        description: |+
          The port of the TCP connection on the client-side

        format: ''
      id:
        type: string
        description: |+
          A server generated ID

        format: ''
  get_admin_status_server_info:
    type: object
    properties:
      progress:
        type: object
        description: |+
          Startup and recovery information.

          You can check for changes to determine whether progress was made between two
          calls, but you should not rely on specific values as they may change between
          ArangoDB versions. The values are only expected to change during the startup and
          shutdown, i.e. while `maintenance` is `true`.

          You need to start _arangod_ with the `--server.early-connections` startup option
          enabled to be able to query the endpoint during the startup process.
          If authentication is enabled, then you need to use the super-user JWT for the
          request because the user management is not available during the startup.

        format: get_admin_status_server_info_progress
      role:
        type: string
        description: |+
          Either `"SINGLE"`, `"COORDINATOR"`, `"PRIMARY"` (DB-Server), or `"AGENT"`.

        format: ''
      writeOpsEnabled:
        type: boolean
        description: |+
          Whether writes are enabled. **Deprecated**, use `readOnly` instead.

        format: ''
      readOnly:
        type: boolean
        description: |+
          Whether writes are disabled.

        format: ''
      maintenance:
        type: boolean
        description: |+
          Whether the maintenance mode is enabled.

        format: ''
      persistedId:
        type: string
        description: |+
          The persisted ID, e. g. `"CRDN-e427b441-5087-4a9a-9983-2fb1682f3e2a"`.
          *Cluster only* (Agents, Coordinators, and DB-Servers).

        format: ''
      rebootId:
        type: number
        description: |+
          The reboot ID. Changes on every restart.
          *Cluster only* (Agents, Coordinators, and DB-Servers).

        format: ''
      state:
        type: string
        description: |+
          Either `"STARTUP"`, `"SERVING"`, or `"SHUTDOWN"`.
          *Cluster only* (Coordinators and DB-Servers).

        format: ''
      address:
        type: string
        description: |+
          The address of the server, e.g. `tcp://[::1]:8530`.
          *Cluster only* (Coordinators and DB-Servers).

        format: ''
      serverId:
        type: string
        description: |+
          The server ID, e.g. `"CRDN-e427b441-5087-4a9a-9983-2fb1682f3e2a"`.
          *Cluster only* (Coordinators and DB-Servers).

        format: ''
  get_admin_status_server_info_progress:
    type: object
    properties:
      phase:
        type: string
        description: |+
          Name of the lifecycle phase the instance is currently in. Normally one of
          `"in prepare"`, `"in start"`, `"in wait"`, `"in shutdown"`, `"in stop"`,
          or `"in unprepare"`.

        format: ''
      feature:
        type: string
        description: |+
          Internal name of the feature that is currently being prepared, started,
          stopped or unprepared.

        format: ''
      recoveryTick:
        type: number
        description: |+
          Current recovery sequence number value, if the instance is currently recovering.
          If the instance is already past the recovery, this attribute will contain the
          last handled recovery sequence number.

        format: ''
  get_admin_status_agency:
    type: object
    properties:
      agencyComm:
        type: object
        description: |+
          Information about the communication with the Agency.
          *Cluster only* (Coordinators and DB-Servers).

        format: get_admin_status_agency_comm
  get_admin_status_agency_comm:
    type: object
    properties:
      endpoints:
        type: array
        description: |+
          A list of possible Agency endpoints.

        items:
          type: string
  get_admin_status_coordinator:
    type: object
    properties:
      foxxmaster:
        type: array
        description: |+
          The server ID of the Coordinator that is the Foxx master.

        items:
          type: string
      isFoxxmaster:
        type: array
        description: |+
          Whether the queried Coordinator is the Foxx master.

        items:
          type: string
  get_admin_status_agent:
    type: object
    properties:
      id:
        type: string
        description: |+
          Server ID of the queried Agent.

        format: ''
      leaderId:
        type: string
        description: |+
          Server ID of the leading Agent.

        format: ''
      leading:
        type: boolean
        description: |+
          Whether the queried Agent is the leader.

        format: ''
      endpoint:
        type: string
        description: |+
          The endpoint of the queried Agent.

        format: ''
  license_features:
    type: object
    properties:
      expires:
        type: number
        description: |+
          The `expires` key lists the expiry date as Unix timestamp (seconds since
          January 1st, 1970 UTC).

        format: ''
  aql_userfunction_struct:
    type: object
    properties:
      name:
        type: string
        description: |+
          The fully qualified name of the user function

        format: ''
      code:
        type: string
        description: |+
          A string representation of the function body

        format: ''
      isDeterministic:
        type: boolean
        description: |+
          an optional boolean value to indicate whether the function
          results are fully deterministic (function return value solely depends on
          the input value and return value is the same for repeated calls with same
          input). The *isDeterministic* attribute is currently not used but may be
          used later for optimizations.

        format: ''
  jwt_keys_struct:
    type: object
    properties:
      encryption-keys:
        type: array
        description: |+
          An array of objects with the SHA-256 hashes of the key secrets.
          Can be empty.

        items:
          type: object
  get_admin_cluster_rebalance_result:
    type: object
    properties:
      leader:
        type: object
        description: |+
          Information about the leader imbalance.

        format: leader_imbalance_struct
      shards:
        type: object
        description: |+
          Information about the shard imbalance.

        format: shard_imbalance_struct
  post_api_index_inverted_fields:
    type: object
    properties:
      name:
        type: string
        description: |+
          An attribute path. The `.` character denotes sub-attributes.
          You can expand one array attribute with `[*]`.

        format: ''
      analyzer:
        type: string
        description: |+
          The name of an Analyzer to use for this field.

          Default: the value defined by the top-level `analyzer` option.

        format: ''
      features:
        type: array
        description: |+
          A list of Analyzer features to use for this field. You can set this option to
          overwrite what features are enabled for the `analyzer`. Possible features:
          - `"frequency"`
          - `"norm"`
          - `"position"`
          - `"offset"`

          Default: the features as defined by the Analyzer itself, or inherited from the
          top-level `features` option if the `analyzer` option adjacent to this option is
          not set.

        items:
          type: string
      includeAllFields:
        type: boolean
        description: |+
          This option only applies if you use the inverted index in a `search-alias` Views.

          If set to `true`, then all sub-attributes of this field are indexed, excluding
          any sub-attributes that are configured separately by other elements in the
          `fields` array (and their sub-attributes). The `analyzer` and `features`
          properties apply to the sub-attributes.

          If set to `false`, then sub-attributes are ignored.

          Default: the value defined by the top-level `includeAllFields` option.

        format: ''
      searchField:
        type: boolean
        description: |+
          This option only applies if you use the inverted index in a `search-alias` Views.

          You can set the option to `true` to get the same behavior as with `arangosearch`
          Views regarding the indexing of array values for this field. If enabled, both,
          array and primitive values (strings, numbers, etc.) are accepted. Every element
          of an array is indexed according to the `trackListPositions` option.

          If set to `false`, it depends on the attribute path. If it explicitly expands an
          array (`[*]`), then the elements are indexed separately. Otherwise, the array is
          indexed as a whole, but only `geopoint` and `aql` Analyzers accept array inputs.
          You cannot use an array expansion if `searchField` is enabled.

          Default: the value defined by the top-level `searchField` option.

        format: ''
      trackListPositions:
        type: boolean
        description: |+
          This option only applies if you use the inverted index in a `search-alias` Views.

          If set to `true`, then track the value position in arrays for array values.
          For example, when querying a document like `{ attr: [ "valueX", "valueY", "valueZ" ] }`,
          you need to specify the array element, e.g. `doc.attr[1] == "valueY"`.

          If set to `false`, all values in an array are treated as equal alternatives.
          You don't specify an array element in queries, e.g. `doc.attr == "valueY"`, and
          all elements are searched for a match.

          Default: the value defined by the top-level `trackListPositions` option.

        format: ''
      nested:
        type: array
        description: |+
          Index the specified sub-objects that are stored in an array. Other than with the
          `fields` property, the values get indexed in a way that lets you query for
          co-occurring values. For example, you can search the sub-objects and all the
          conditions need to be met by a single sub-object instead of across all of them.

          This property is available in the Enterprise Edition only.

        items:
          $ref: '#/components/schemas/post_api_index_inverted_nested'
  post_api_index_inverted_nested:
    type: object
    properties:
      name:
        type: string
        description: |+
          An attribute path. The `.` character denotes sub-attributes.

        format: ''
      analyzer:
        type: string
        description: |+
          The name of an Analyzer to use for this field.
          Default: the value defined by the parent field, or the top-level `analyzer` option.

        format: ''
      features:
        type: array
        description: |+
          A list of Analyzer features to use for this field. You can set this option to
          overwrite what features are enabled for the `analyzer`. Possible features:
          - `"frequency"`
          - `"norm"`
          - `"position"`
          - `"offset"`

          Default: the features as defined by the Analyzer itself, or inherited from the
          the parent field's or top-level `features` option if no `analyzer` option is set
          at a deeper level, closer to this option.

        items:
          type: string
      searchField:
        type: boolean
        description: |+
          This option only applies if you use the inverted index in a `search-alias` Views.

          You can set the option to `true` to get the same behavior as with `arangosearch`
          Views regarding the indexing of array values for this field. If enabled, both,
          array and primitive values (strings, numbers, etc.) are accepted. Every element
          of an array is indexed according to the `trackListPositions` option.

          If set to `false`, it depends on the attribute path. If it explicitly expands an
          array (`[*]`), then the elements are indexed separately. Otherwise, the array is
          indexed as a whole, but only `geopoint` and `aql` Analyzers accept array inputs.
          You cannot use an array expansion if `searchField` is enabled.

          Default: the value defined by the top-level `searchField` option.

        format: ''
      nested:
        type: array
        description: |+
          You can recursively index sub-objects. See the above description of the
          `nested` option.

        items:
          type: object
  post_api_index_inverted_storedvalues:
    type: object
    properties:
      fields:
        type: array
        description: |+
          A list of attribute paths. The `.` character denotes sub-attributes.

        items:
          type: string
      compression:
        type: string
        description: |+
          Defines how to compress the attribute values. Possible values:
          - `"lz4"` (default): use LZ4 fast compression.
          - `"none"`: disable compression to trade space for speed.

        format: ''
  post_api_index_inverted_primarysort:
    type: object
    properties:
      fields:
        type: array
        description: |+
          An array of the fields to sort the index by and the direction to sort each field in.

        items:
          $ref: '#/components/schemas/post_api_index_inverted_primarysort_fields'
      compression:
        type: string
        description: |+
          Defines how to compress the primary sort data. Possible values:
          - `"lz4"` (default): use LZ4 fast compression.
          - `"none"`: disable compression to trade space for speed.

        format: ''
  post_api_index_inverted_primarysort_fields:
    type: object
    properties:
      field:
        type: string
        description: |+
          An attribute path. The `.` character denotes sub-attributes.

        format: ''
      direction:
        type: string
        description: |+
          The sorting direction. Possible values:
          - `"asc` for ascending
          - `"desc"` for descending

        format: ''
  post_api_index_inverted_policy:
    type: object
    properties:
      type:
        type: string
        description: |+
          The segment candidates for the "consolidation" operation are selected based
          upon several possible configurable formulas as defined by their types.
          The supported types are:

          - `"tier"` (default): consolidate based on segment byte size and live
            document count as dictated by the customization attributes.

        format: ''
      segmentsBytesFloor:
        type: integer
        description: |+
          Defines the value (in bytes) to treat all smaller segments as equal for
          consolidation selection. Default: `2097152`

        format: ''
      segmentsBytesMax:
        type: integer
        description: |+
          The maximum allowed size of all consolidated segments in bytes.
          Default: `5368709120`

        format: ''
      segmentsMax:
        type: integer
        description: |+
          The maximum number of segments that are evaluated as candidates for
          consolidation. Default: `10`

        format: ''
      segmentsMin:
        type: integer
        description: |+
          The minimum number of segments that are evaluated as candidates for
          consolidation. Default: `1`

        format: ''
      minScore:
        type: integer
        description: |+
          Filter out consolidation candidates with a score less than this. Default: `0`

        format: ''
  put_api_collection_properties_computed_field:
    type: object
    properties:
      name:
        type: string
        description: |+
          The name of the target attribute. Can only be a top-level attribute, but you
          may return a nested object. Cannot be `_key`, `_id`, `_rev`, `_from`, `_to`,
          or a shard key attribute.

        format: ''
      expression:
        type: string
        description: |+
          An AQL `RETURN` operation with an expression that computes the desired value.
          See [Computed Value Expressions](https://www.arangodb.com/docs/devel/data-modeling-documents-computed-values.html#computed-value-expressions) for details.

        format: ''
      overwrite:
        type: boolean
        description: |+
          Whether the computed value shall take precedence over a user-provided or
          existing attribute.

        format: ''
      computeOn:
        type: array
        description: |+
          An array of strings to define on which write operations the value shall be
          computed. The possible values are `"insert"`, `"update"`, and `"replace"`.
          The default is `["insert", "update", "replace"]`.

        items:
          type: string
      keepNull:
        type: boolean
        description: |+
          Whether the target attribute shall be set if the expression evaluates to `null`.
          You can set the option to `false` to not set (or unset) the target attribute if
          the expression returns `null`. The default is `true`.

        format: ''
      failOnWarning:
        type: boolean
        description: |+
          Whether to let the write operation fail if the expression produces a warning.
          The default is `false`.

        format: ''
  post_api_collection_computed_field:
    type: object
    properties:
      name:
        type: string
        description: |+
          The name of the target attribute. Can only be a top-level attribute, but you
          may return a nested object. Cannot be `_key`, `_id`, `_rev`, `_from`, `_to`,
          or a shard key attribute.

        format: ''
      expression:
        type: string
        description: |+
          An AQL `RETURN` operation with an expression that computes the desired value.
          See [Computed Value Expressions](https://www.arangodb.com/docs/devel/data-modeling-documents-computed-values.html#computed-value-expressions) for details.

        format: ''
      overwrite:
        type: boolean
        description: |+
          Whether the computed value shall take precedence over a user-provided or
          existing attribute.

        format: ''
      computeOn:
        type: array
        description: |+
          An array of strings to define on which write operations the value shall be
          computed. The possible values are `"insert"`, `"update"`, and `"replace"`.
          The default is `["insert", "update", "replace"]`.

        items:
          type: string
      keepNull:
        type: boolean
        description: |+
          Whether the target attribute shall be set if the expression evaluates to `null`.
          You can set the option to `false` to not set (or unset) the target attribute if
          the expression returns `null`. The default is `true`.

        format: ''
      failOnWarning:
        type: boolean
        description: |+
          Whether to let the write operation fail if the expression produces a warning.
          The default is `false`.

        format: ''
  post_api_collection_opts:
    type: object
    properties:
      type:
        type: string
        description: |+
          specifies the type of the key generator. The currently available generators are
          `traditional`, `autoincrement`, `uuid` and `padded`.

          - The `traditional` key generator generates numerical keys in ascending order.
            The sequence of keys is not guaranteed to be gap-free.

          - The `autoincrement` key generator generates numerical keys in ascending order,
            the initial offset and the spacing can be configured (**note**: `autoincrement`
            is currently only supported for non-sharded collections).
            The sequence of generated keys is not guaranteed to be gap-free, because a new key
            will be generated on every document insert attempt, not just for successful
            inserts.

          - The `padded` key generator generates keys of a fixed length (16 bytes) in
            ascending lexicographical sort order. This is ideal for usage with the _RocksDB_
            engine, which will slightly benefit keys that are inserted in lexicographically
            ascending order. The key generator can be used in a single-server or cluster.
            The sequence of generated keys is not guaranteed to be gap-free.

          - The `uuid` key generator generates universally unique 128 bit keys, which
            are stored in hexadecimal human-readable format. This key generator can be used
            in a single-server or cluster to generate "seemingly random" keys. The keys
            produced by this key generator are not lexicographically sorted.

          Please note that keys are only guaranteed to be truly ascending in single
          server deployments and for collections that only have a single shard (that includes
          collections in a OneShard database).
          The reason is that for collections with more than a single shard, document keys
          are generated on coordinator(s). For collections with a single shard, the document
          keys are generated on the leader DB server, which has full control over the key
          sequence.

      allowUserKeys:
        type: boolean
        description: |+
          If set to `true`, then you are allowed to supply own key values in the
          `_key` attribute of documents. If set to `false`, then the key generator
          is solely be responsible for generating keys and an error is raised if you
          supply own key values in the `_key` attribute of documents.

        format: ''
      increment:
        type: integer
        description: |+
          increment value for `autoincrement` key generator. Not used for other key
          generator types.

        format: int64
      offset:
        type: integer
        description: |+
          Initial offset value for `autoincrement` key generator.
          Not used for other key generator types.

        format: int64
  collection_figures:
    type: object
    properties:
      indexes:
        type: object
        description: |2+

        format: collection_figures_indexes
  collection_figures_indexes:
    type: object
    properties:
      count:
        type: integer
        description: |+
          The total number of indexes defined for the collection, including the pre-defined
          indexes (e.g. primary index).

        format: int64
      size:
        type: integer
        description: |+
          The total memory allocated for indexes in bytes.

        format: int64
  get_api_database_new_OPTIONS:
    type: object
    properties:
      sharding:
        type: string
        description: |+
          The sharding method to use for new collections in this database. Valid values
          are: "", "flexible", or "single". The first two are equivalent. _(cluster only)_

        format: ''
      replicationFactor:
        type: integer
        description: |+
          Default replication factor for new collections created in this database.
          Special values include "satellite", which will replicate the collection to
          every DB-Server (Enterprise Edition only), and 1, which disables replication.
          _(cluster only)_

        format: ''
      writeConcern:
        type: number
        description: |+
          Default write concern for new collections created in this database.
          It determines how many copies of each shard are required to be
          in sync on the different DB-Servers. If there are less then these many copies
          in the cluster a shard will refuse to write. Writes to shards with enough
          up-to-date copies will succeed at the same time however. The value of
          *writeConcern* can not be larger than *replicationFactor*. _(cluster only)_

        format: ''
  get_api_database_new_USERS:
    type: object
    properties:
      username:
        type: string
        description: |+
          Login name of an existing user or one to be created.

        format: ''
      passwd:
        type: string
        description: |+
          The user password as a string. If not specified, it will default to an empty
          string. The attribute is ignored for users that already exist.

        format: password
      active:
        type: boolean
        description: |+
          A flag indicating whether the user account should be activated or not.
          The default value is *true*. If set to *false*, then the user won't be able to
          log into the database. The default is *true*. The attribute is ignored for users
          that already exist.

        format: ''
      extra:
        type: object
        description: |+
          A JSON object with extra user information. It is used by the web interface
          to store graph viewer settings and saved queries. Should not be set or
          modified by end users, as custom attributes will not be preserved.

        format: ''
  post_api_view_searchalias_indexes:
    type: object
    properties:
      collection:
        type: string
        description: |+
          The name of a collection.

        format: ''
      index:
        type: string
        description: |+
          The name of an inverted index of the `collection`, or the index ID without
          the `<collection>/` prefix.

        format: ''
  put_api_view_searchalias_indexes:
    type: object
    properties:
      collection:
        type: string
        description: |+
          The name of a collection.

        format: ''
      index:
        type: string
        description: |+
          The name of an inverted index of the `collection`, or the index ID without
          the `<collection>/` prefix.

        format: ''
  put_api_view_searchalias_indexes_reply:
    type: object
    properties:
      collection:
        type: string
        description: |+
          The name of a collection.

        format: ''
      index:
        type: string
        description: |+
          The name of an inverted index of the `collection`.

        format: ''
  patch_api_view_searchalias_indexes:
    type: object
    properties:
      collection:
        type: string
        description: |+
          The name of a collection.

        format: ''
      index:
        type: string
        description: |+
          The name of an inverted index of the `collection`, or the index ID without
          the `<collection>/` prefix.

        format: ''
      operation:
        type: string
        description: |+
          Whether to add or remove the index to the stored `indexes` property of the View.
          Possible values: `"add"`, `"del"`. The default is `"add"`.

        format: ''
  patch_api_view_searchalias_indexes_reply:
    type: object
    properties:
      collection:
        type: string
        description: |+
          The name of a collection.

        format: ''
      index:
        type: string
        description: |+
          The name of an inverted index of the `collection`.

        format: ''
  graph_list:
    type: object
    properties:
      graph:
        type: object
        description: |+
          The information about the newly created graph

        format: graph_representation
  graph_representation:
    type: object
    properties:
      name:
        type: string
        description: |+
          The name of the graph

        format: ''
      edgeDefinitions:
        type: array
        description: |+
          An array of definitions for the relations of the graph.
          Each has the following type:

        items:
          $ref: '#/components/schemas/graph_edge_definition'
      orphanCollections:
        type: array
        description: |+
          An array of additional vertex collections.
          Documents within these collections do not have edges within this graph.

        items:
          type: string
      numberOfShards:
        type: integer
        description: |+
          Number of shards created for every new collection in the graph.

        format: ''
      _id:
        type: string
        description: |+
          The internal id value of this graph.

        format: ''
      _rev:
        type: string
        description: |+
          The revision of this graph. Can be used to make sure to not override
          concurrent modifications to this graph.

        format: ''
      replicationFactor:
        type: integer
        description: |+
          The replication factor used for every new collection in the graph.
          Can also be the string `"satellite"` for a SmartGraph (Enterprise Edition only).

        format: ''
      isSmart:
        type: boolean
        description: |+
          Whether the graph is a SmartGraph (Enterprise Edition only).

        format: ''
      isDisjoint:
        type: boolean
        description: |+
          Whether the graph is a Disjoint SmartGraph (Enterprise Edition only).

        format: ''
      smartGraphAttribute:
        type: string
        description: |+
          Name of the sharding attribute in the SmartGraph case (Enterprise Edition only).

        format: ''
  post_api_gharial_create_opts:
    type: object
    properties:
      smartGraphAttribute:
        type: string
        description: |+
          Only has effect in Enterprise Edition and it is required if isSmart is true.
          The attribute name that is used to smartly shard the vertices of a graph.
          Every vertex in this SmartGraph has to have this attribute.
          Cannot be modified later.

        format: ''
      satellites:
        type: array
        description: |+
          An array of collection names that is used to create SatelliteCollections
          for a (Disjoint) SmartGraph using SatelliteCollections (Enterprise Edition only).
          Each array element must be a string and a valid collection name.
          The collection type cannot be modified later.

        items:
          type: string
      numberOfShards:
        type: integer
        description: |+
          The number of shards that is used for every collection within this graph.
          Cannot be modified later.

        format: ''
      replicationFactor:
        type: integer
        description: |+
          The replication factor used when initially creating collections for this graph.
          Can be set to `"satellite"` to create a SatelliteGraph, which will ignore
          *numberOfShards*, *minReplicationFactor* and *writeConcern*
          (Enterprise Edition only).

        format: ''
      writeConcern:
        type: integer
        description: |+
          Write concern for new collections in the graph.
          It determines how many copies of each shard are required to be
          in sync on the different DB-Servers. If there are less then these many copies
          in the cluster a shard will refuse to write. Writes to shards with enough
          up-to-date copies will succeed at the same time however. The value of
          *writeConcern* can not be larger than *replicationFactor*. _(cluster only)_

        format: ''
  post_api_vertex_create_opts:
    type: object
    properties:
      satellites:
        type: array
        description: |+
          An array of collection names that is used to create SatelliteCollections
          for a (Disjoint) SmartGraph using SatelliteCollections (Enterprise Edition only).
          Each array element must be a string and a valid collection name.
          The collection type cannot be modified later.

        items:
          type: string
  post_api_edgedef_create_opts:
    type: object
    properties:
      satellites:
        type: array
        description: |+
          An array of collection names that is used to create SatelliteCollections
          for a (Disjoint) SmartGraph using SatelliteCollections (Enterprise Edition only).
          Each array element must be a string and a valid collection name.
          The collection type cannot be modified later.

        items:
          type: string
  post_api_edgedef_modify_opts:
    type: object
    properties:
      satellites:
        type: array
        description: |+
          An array of collection names that is used to create SatelliteCollections
          for a (Disjoint) SmartGraph using SatelliteCollections (Enterprise Edition only).
          Each array element must be a string and a valid collection name.
          The collection type cannot be modified later.

        items:
          type: string
  post_api_cursor_opts:
    type: object
    properties:
      fullCount:
        type: boolean
        description: |+
          if set to *true* and the query contains a *LIMIT* clause, then the
          result will have an *extra* attribute with the sub-attributes *stats*
          and *fullCount*, `{ ... , "extra": { "stats": { "fullCount": 123 } } }`.
          The *fullCount* attribute will contain the number of documents in the result before the
          last top-level LIMIT in the query was applied. It can be used to count the number of
          documents that match certain filter criteria, but only return a subset of them, in one go.
          It is thus similar to MySQL's *SQL_CALC_FOUND_ROWS* hint. Note that setting the option
          will disable a few LIMIT optimizations and may lead to more documents being processed,
          and thus make queries run longer. Note that the *fullCount* attribute may only
          be present in the result if the query has a top-level LIMIT clause and the LIMIT
          clause is actually used in the query.

        format: ''
      fillBlockCache:
        type: boolean
        description: "if set to *true* or not specified, this will make the query\
          \ store the data it \nreads via the RocksDB storage engine in the RocksDB\
          \ block cache. This is usually \nthe desired behavior. The option can be\
          \ set to *false* for queries that are\nknown to either read a lot of data\
          \ which would thrash the block cache, or for queries\nthat read data which\
          \ are known to be outside of the hot set. By setting the option\nto *false*,\
          \ data read by the query will not make it into the RocksDB block cache if\n\
          not already in there, thus leaving more room for the actual hot set.\n\n"
        format: ''
      maxPlans:
        type: integer
        description: |+
          Limits the maximum number of plans that are created by the AQL query optimizer.

        format: int64
      maxNodesPerCallstack:
        type: integer
        description: |+
          The number of execution nodes in the query plan after that stack splitting is
          performed to avoid a potential stack overflow. Defaults to the configured value
          of the startup option `--query.max-nodes-per-callstack`.

          This option is only useful for testing and debugging and normally does not need
          any adjustment.

        format: int64
      maxWarningCount:
        type: integer
        description: |+
          Limits the maximum number of warnings a query will return. The number of warnings
          a query will return is limited to 10 by default, but that number can be increased
          or decreased by setting this attribute.

        format: int64
      failOnWarning:
        type: boolean
        description: |+
          When set to *true*, the query will throw an exception and abort instead of producing
          a warning. This option should be used during development to catch potential issues
          early. When the attribute is set to *false*, warnings will not be propagated to
          exceptions and will be returned with the query result.
          There is also a server configuration option `--query.fail-on-warning` for setting the
          default value for *failOnWarning* so it does not need to be set on a per-query level.

        format: ''
      stream:
        type: boolean
        description: |+
          Can be enabled to execute the query lazily. If set to *true*, then the query is
          executed as long as necessary to produce up to `batchSize` results. These
          results are returned immediately and the query is suspended until the client
          asks for the next batch (if there are more results). Depending on the query
          this can mean that the first results will be available much faster and that
          less memory is needed because the server only needs to store a subset of
          results at a time. Read-only queries can benefit the most, unless `SORT`
          without index or `COLLECT` are involved that make it necessary to process all
          documents before a partial result can be returned. It is advisable to only use
          this option for queries without exclusive locks.

          Remarks:
          - The query will hold resources until it ends (such as RocksDB snapshots, which
            prevents compaction to some degree). Writes will be in memory until the query
            is committed.
          - If existing documents are modified, then write locks are held on these
            documents and other queries trying to modify the same documents will fail
            because of this conflict.
          - A streaming query may fail late because of a conflict or for other reasons
            after some batches were already returned successfully, possibly rendering the
            results up to that point meaningless.
          - The query options `cache`, `count` and `fullCount` are not supported for
            streaming queries.
          - Query statistics, profiling data and warnings are delivered as part of the
            last batch.

          If the `stream` option is *false* (default), then the complete result of the
          query is calculated before any of it is returned to the client. The server
          stores the full result in memory (on the contacted Coordinator if in a cluster).
          All other resources are freed immediately (locks, RocksDB snapshots). The query
          will fail before it returns results in case of a conflict.

        format: ''
      optimizer:
        type: object
        description: |+
          Options related to the query optimizer.

        format: post_api_cursor_opts_optimizer
      profile:
        type: integer
        description: |+
          If set to `true` or `1`, then the additional query profiling information is returned
          in the `profile` sub-attribute of the `extra` return attribute, unless the query result
          is served from the query cache. If set to `2`, the query includes execution stats
          per query plan node in `stats.nodes` sub-attribute of the `extra` return attribute.
          Additionally, the query plan is returned in the `extra.plan` sub-attribute.

        format: ''
      satelliteSyncWait:
        type: number
        description: |+
          This *Enterprise Edition* parameter allows to configure how long a DB-Server has time
          to bring the SatelliteCollections involved in the query into sync.
          The default value is `60.0` seconds. When the maximal time is reached, the query
          is stopped.

        format: double
      maxRuntime:
        type: number
        description: |+
          The query has to be executed within the given runtime or it is killed.
          The value is specified in seconds. The default value is `0.0` (no timeout).

        format: double
      maxTransactionSize:
        type: integer
        description: |+
          The transaction size limit in bytes.

        format: int64
      intermediateCommitSize:
        type: integer
        description: |+
          The maximum total size of operations after which an intermediate commit is performed
          automatically.

        format: int64
      intermediateCommitCount:
        type: integer
        description: |+
          The maximum number of operations after which an intermediate commit is performed
          automatically.

        format: int64
      skipInaccessibleCollections:
        type: boolean
        description: |+
          Let AQL queries (especially graph traversals) treat collection to which a user
          has no access rights for as if these collections are empty. Instead of returning a
          forbidden access error, your queries execute normally. This is intended to help
          with certain use-cases: A graph contains several collections and different users
          execute AQL queries on that graph. You can naturally limit the accessible
          results by changing the access rights of users on collections.

          This feature is only available in the Enterprise Edition.

        format: ''
  post_api_cursor_opts_optimizer:
    type: object
    properties:
      rules:
        type: array
        description: |+
          A list of to-be-included or to-be-excluded optimizer rules can be put into this
          attribute, telling the optimizer to include or exclude specific rules. To disable
          a rule, prefix its name with a `-`, to enable a rule, prefix it with a `+`. There is
          also a pseudo-rule `all`, which matches all optimizer rules. `-all` disables all rules.

        items:
          type: string
  post_api_cursor_extra:
    type: object
    properties:
      warnings:
        type: array
        description: |+
          A list of query warnings.

        items:
          $ref: '#/components/schemas/post_api_cursor_extra_warnings'
      stats:
        type: object
        description: |+
          An object with query statistics.

        format: post_api_cursor_extra_stats
      profile:
        type: object
        description: |+
          The duration of the different query execution phases in seconds.

        format: post_api_cursor_extra_profile
      plan:
        type: object
        description: |+
          The execution plan.

        format: post_api_cursor_extra_plan
  post_api_cursor_extra_warnings:
    type: object
    properties:
      code:
        type: integer
        description: |+
          An error code.

        format: ''
      message:
        type: string
        description: |+
          A description of the problem.

        format: ''
  post_api_cursor_extra_stats:
    type: object
    properties:
      writesExecuted:
        type: integer
        description: |+
          The total number of data-modification operations successfully executed.

        format: ''
      writesIgnored:
        type: integer
        description: |+
          The total number of data-modification operations that were unsuccessful,
          but have been ignored because of the `ignoreErrors` query option.

        format: ''
      scannedFull:
        type: integer
        description: "The total number of documents iterated over when scanning a\
          \ collection \nwithout an index. Documents scanned by subqueries are included\
          \ in the result, but\noperations triggered by built-in or user-defined AQL\
          \ functions are not.\n\n"
        format: ''
      scannedIndex:
        type: integer
        description: |+
          The total number of documents iterated over when scanning a collection using
          an index. Documents scanned by subqueries are included in the result, but operations
          triggered by built-in or user-defined AQL functions are not.

        format: ''
      cursorsCreated:
        type: integer
        description: |+
          The total number of cursor objects created during query execution. Cursor
          objects are created for index lookups.

        format: ''
      cursorsRearmed:
        type: integer
        description: |+
          The total number of times an existing cursor object was repurposed.
          Repurposing an existing cursor object is normally more efficient compared to destroying an
          existing cursor object and creating a new one from scratch.

        format: ''
      cacheHits:
        type: integer
        description: |+
          The total number of index entries read from in-memory caches for indexes
          of type edge or persistent. This value is only non-zero when reading from indexes
          that have an in-memory cache enabled, and when the query allows using the in-memory
          cache (i.e. using equality lookups on all index attributes).

        format: ''
      cacheMisses:
        type: integer
        description: |+
          The total number of cache read attempts for index entries that could not
          be served from in-memory caches for indexes of type edge or persistent. This value
          is only non-zero when reading from indexes that have an in-memory cache enabled, the
          query allows using the in-memory cache (i.e. using equality lookups on all index attributes)
          and the looked up values are not present in the cache.

        format: ''
      filtered:
        type: integer
        description: "The total number of documents removed after executing a filter\
          \ condition\nin a `FilterNode` or another node that post-filters data. Note\
          \ that nodes of the\n`IndexNode` type can also filter documents by selecting\
          \ only the required index range \nfrom a collection, and the `filtered`\
          \ value only indicates how much filtering was done by a\npost filter in\
          \ the `IndexNode` itself or following `FilterNode` nodes.\nNodes of the\
          \ `EnumerateCollectionNode` and `TraversalNode` types can also apply\nfilter\
          \ conditions and can report the number of filtered documents.\n\n"
        format: ''
      httpRequests:
        type: integer
        description: |+
          The total number of cluster-internal HTTP requests performed.

        format: ''
      fullCount:
        type: integer
        description: "The total number of documents that matched the search condition\
          \ if the query's\nfinal top-level `LIMIT` operation were not present.\n\
          This attribute may only be returned if the `fullCount` option was set when\
          \ starting the \nquery and only contains a sensible value if the query contains\
          \ a `LIMIT` operation on\nthe top level.\n\n"
        format: ''
      executionTime:
        type: number
        description: |+
          The query execution time (wall-clock time) in seconds.

        format: ''
      peakMemoryUsage:
        type: integer
        description: "The maximum memory usage of the query while it was running.\
          \ In a cluster,\nthe memory accounting is done per shard, and the memory\
          \ usage reported is the peak\nmemory usage value from the individual shards.\n\
          Note that to keep things lightweight, the per-query memory usage is tracked\
          \ on a relatively \nhigh level, not including any memory allocator overhead\
          \ nor any memory used for temporary\nresults calculations (e.g. memory allocated/deallocated\
          \ inside AQL expressions and function \ncalls).\n\n"
        format: ''
      nodes:
        type: array
        description: |+
          When the query is executed with the `profile` option set to at least `2`,
          then this attribute contains runtime statistics per query execution node.
          For a human readable output, you can execute
          `db._profileQuery(<query>, <bind-vars>)` in arangosh.

        items:
          $ref: '#/components/schemas/post_api_cursor_extra_stats_nodes'
  post_api_cursor_extra_stats_nodes:
    type: object
    properties:
      id:
        type: integer
        description: |+
          The execution node ID to correlate the statistics with the `plan` returned in
          the `extra` attribute.

        format: ''
      calls:
        type: integer
        description: |+
          The number of calls to this node.

        format: ''
      items:
        type: integer
        description: |+
          The number of items returned by this node. Items are the temporary results
          returned at this stage.

        format: ''
      runtime:
        type: number
        description: |+
          The execution time of this node in seconds.

        format: ''
  post_api_cursor_extra_profile:
    type: object
    properties:
      initializing:
        type: number
        description: ''
        format: ''
      parsing:
        type: number
        description: ''
        format: ''
      optimizing ast:
        type: number
        description: ''
        format: ''
      loading collections:
        type: number
        description: ''
        format: ''
      instantiating plan:
        type: number
        description: ''
        format: ''
      optimizing plan:
        type: number
        description: ''
        format: ''
      executing:
        type: number
        description: ''
        format: ''
      finalizing:
        type: number
        description: |2+

        format: ''
  post_api_cursor_extra_plan:
    type: object
    properties:
      nodes:
        type: array
        description: |+
          A nested list of the execution plan nodes.

        items:
          type: object
      rules:
        type: array
        description: |+
          A list with the names of the applied optimizer rules.

        items:
          type: string
      collections:
        type: array
        description: |+
          A list of the collections involved in the query. The list only includes the
          collections that can statically be determined at query compile time.

        items:
          $ref: '#/components/schemas/post_api_cursor_extra_plan_collections'
      variables:
        type: array
        description: |+
          All of the query variables, including user-created and internal ones.

        items:
          type: object
      estimatedCost:
        type: integer
        description: |+
          The estimated cost of the query.

        format: ''
      estimatedNrItems:
        type: integer
        description: |+
          The estimated number of results.

        format: ''
      isModificationQuery:
        type: boolean
        description: |+
          Whether the query contains write operations.

        format: ''
  post_api_cursor_extra_plan_collections:
    type: object
    properties:
      name:
        type: string
        description: |+
          The collection name.

        format: ''
      type:
        type: string
        description: |+
          How the collection is used. Can be `"read"`, `"write"`, or `"exclusive"`.

        format: ''
  get_cluster_maintenance_dbserver_result:
    type: object
    properties:
      Mode:
        type: string
        description: |+
          The mode of the DB-Server. The value is `"maintenance"`.

        format: ''
      Until:
        type: string
        description: |+
          Until what date and time the maintenance mode currently lasts, in the
          ISO 8601 date/time format.

        format: dateTime
parameters: []
securitySchemes: []
requestBodies: []
responses: []
headers: []
links: []
callbacks: []
